{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5b4de39-1ac0-4857-bf8d-df6791da9d07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import zuko\n",
    "import torch\n",
    "\n",
    "from helpers.distributions import *\n",
    "from helpers.flow import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "608dff40-b8c9-441c-9c34-2b9e0aad3746",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 'C_alpha_1', 'LOSS': 'linearMSE', 'MODE': 'UNIFORM_SAMPLES', 'architecture': {'hidden_features': [64, 64], 'num_aux': 1, 'num_trainable_params': 51842, 'num_transforms': 7}, 'hyperparams': {'batch_num_c': 32, 'batch_num_x': 512, 'epochs': 2000, 'lr': 0.001, 'seed': 1}, 'physics': {'E0': 500, 'R': 0.4, 'target_p': '<function LO_angularity at 0x7f1889803670>'}}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "flow_id = \"Calpha1_linear\"\n",
    "\n",
    "with open(f\"test/{flow_id}.yml\") as stream:\n",
    "    run_params = yaml.safe_load(stream)\n",
    "print(run_params)\n",
    "\n",
    "target_p = LO_angularity\n",
    "x_range = (0, 1)\n",
    "\n",
    "E0 = run_params[\"physics\"][\"E0\"]\n",
    "R = run_params[\"physics\"][\"R\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f7779cf-2e23-4b24-a197-a947e061d737",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/rmastand/.local/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NSF(\n",
       "  (transform): LazyComposedTransform(\n",
       "    (0): MaskedAutoregressiveTransform(\n",
       "      (base): MonotonicRQSTransform(bins=8)\n",
       "      (order): [0, 1]\n",
       "      (hyper): MaskedMLP(\n",
       "        (0): MaskedLinear(in_features=3, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): MaskedLinear(in_features=64, out_features=64, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): MaskedLinear(in_features=64, out_features=46, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): MaskedAutoregressiveTransform(\n",
       "      (base): MonotonicRQSTransform(bins=8)\n",
       "      (order): [1, 0]\n",
       "      (hyper): MaskedMLP(\n",
       "        (0): MaskedLinear(in_features=3, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): MaskedLinear(in_features=64, out_features=64, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): MaskedLinear(in_features=64, out_features=46, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (2): MaskedAutoregressiveTransform(\n",
       "      (base): MonotonicRQSTransform(bins=8)\n",
       "      (order): [0, 1]\n",
       "      (hyper): MaskedMLP(\n",
       "        (0): MaskedLinear(in_features=3, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): MaskedLinear(in_features=64, out_features=64, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): MaskedLinear(in_features=64, out_features=46, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (3): MaskedAutoregressiveTransform(\n",
       "      (base): MonotonicRQSTransform(bins=8)\n",
       "      (order): [1, 0]\n",
       "      (hyper): MaskedMLP(\n",
       "        (0): MaskedLinear(in_features=3, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): MaskedLinear(in_features=64, out_features=64, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): MaskedLinear(in_features=64, out_features=46, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (4): MaskedAutoregressiveTransform(\n",
       "      (base): MonotonicRQSTransform(bins=8)\n",
       "      (order): [0, 1]\n",
       "      (hyper): MaskedMLP(\n",
       "        (0): MaskedLinear(in_features=3, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): MaskedLinear(in_features=64, out_features=64, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): MaskedLinear(in_features=64, out_features=46, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (5): MaskedAutoregressiveTransform(\n",
       "      (base): MonotonicRQSTransform(bins=8)\n",
       "      (order): [1, 0]\n",
       "      (hyper): MaskedMLP(\n",
       "        (0): MaskedLinear(in_features=3, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): MaskedLinear(in_features=64, out_features=64, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): MaskedLinear(in_features=64, out_features=46, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (6): MaskedAutoregressiveTransform(\n",
       "      (base): MonotonicRQSTransform(bins=8)\n",
       "      (order): [0, 1]\n",
       "      (hyper): MaskedMLP(\n",
       "        (0): MaskedLinear(in_features=3, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): MaskedLinear(in_features=64, out_features=64, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): MaskedLinear(in_features=64, out_features=46, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (base): UnconditionalDistribution(DiagNormal(loc: torch.Size([2]), scale: torch.Size([2])))\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auxiliary_params = run_params[\"architecture\"][\"num_aux\"]\n",
    "\n",
    "flow = zuko.flows.NSF(features = auxiliary_params + 1, context=1, transforms= run_params[\"architecture\"][\"num_transforms\"], hidden_features=run_params[\"architecture\"][\"hidden_features\"])\n",
    "flow.load_state_dict(torch.load(f\"test/{flow_id}_model\", weights_only=True))\n",
    "flow.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cb1298-d894-4494-b9c1-d4ef9952de45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot distribution of xs\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "cs = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.999][::-1]\n",
    "# cs = [0.5]\n",
    "\n",
    "DRAW_NF_HIST = False\n",
    "DRAW_NF_LINES = True\n",
    "aux_draws = 5\n",
    "\n",
    "\n",
    "for i in range(len(cs)):\n",
    "\n",
    "\n",
    "    # Draw hist\n",
    "    c = torch.tensor((cs[i],))\n",
    "\n",
    "    if DRAW_NF_HIST:\n",
    "        samples = sigmoid(flow(c).sample((100000,)))\n",
    "        x_samples = samples[:,0].detach().numpy()\n",
    "        aux_samples = samples[:,1:].detach().numpy()\n",
    "\n",
    "        jet_color = plt.get_cmap(\"jet\")(  i / 10)\n",
    "        if i == 0:\n",
    "            label = \"Generated Samples\"\n",
    "        else:\n",
    "            label = None\n",
    "        ax.hist(x_samples, bins=100, density=True, color = jet_color, alpha = 0.5, label = label, histtype=\"step\")\n",
    "\n",
    "    if DRAW_NF_LINES:\n",
    "\n",
    "        for aux_draw in range(aux_draws):\n",
    "            xs = torch.rand(10000, auxiliary_params + 1) * (x_range[1] - x_range[0]) + x_range[0]\n",
    "            \n",
    "            # Sort the first column\n",
    "            xs = xs[torch.argsort(xs[:,0])]\n",
    "            xs[:,1:] = torch.rand(1) * torch.ones((10000, auxiliary_params))\n",
    "            \n",
    "\n",
    "            logJ = torch.sum(log_abs_det_jacobian_sigmoid(inverse_sigmoid(xs)), axis = 1)\n",
    "            ys = (flow(c).log_prob(inverse_sigmoid(xs)) + logJ).exp()\n",
    "\n",
    "            xs = xs[:,0].detach().numpy().flatten()\n",
    "            ys = np.nan_to_num(ys.detach().numpy().flatten())\n",
    "\n",
    "            jet_color = plt.get_cmap(\"jet\")( i / 10)\n",
    "            if i == 0 and aux_draw == 0:\n",
    "                label = \"Learned Flow\"\n",
    "            else:\n",
    "                label = None\n",
    "\n",
    "            ax.plot(xs, ys, color = jet_color, lw = 1, alpha = 0.5, label = label)\n",
    "\n",
    "            ax.vlines(cs[i], 0, target_p(c, E0, R), color = jet_color, linestyle = \"--\", alpha = 0.25, lw = 0.5)\n",
    "\n",
    "\n",
    "\n",
    "# Add a colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap=plt.get_cmap(\"jet_r\"), norm=plt.Normalize(vmin=0, vmax=1))\n",
    "sm.set_array([])\n",
    "plt.colorbar(sm, label=\"Cutoff\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# c = torch.tensor((0.5,))\n",
    "# xs = torch.zeros((10000, auxiliary_params + 1)) + 0.5\n",
    "# xs[:,0] = torch.linspace(0.001, 0.999, 10000)\n",
    "# logJ = torch.sum(log_abs_det_jacobian_sigmoid(inverse_sigmoid(xs)), axis = 1)\n",
    "# ys = (flow(c).log_prob(inverse_sigmoid(xs)) + logJ).exp()\n",
    "\n",
    "# print(flow(c).log_prob(inverse_sigmoid(xs)))\n",
    "# print(ys)\n",
    "\n",
    "# xs = xs[:,0].detach().numpy().flatten()\n",
    "# ys = np.nan_to_num(ys.detach().numpy().flatten())\n",
    "# ps = target_p(torch.tensor(xs)).detach().numpy().flatten()\n",
    "# plt.plot(xs, ys, color = \"red\", label = \"Learned Flow\")\n",
    "# print(ys.mean())\n",
    "\n",
    "# c = c.detach().numpy()\n",
    "\n",
    "cutoff = xs > 0.5\n",
    "\n",
    "plt.plot(xs, target_p(torch.tensor(xs), E0, R), color = \"Black\", label = \"Target (Uncut)\")\n",
    "plt.plot(xs, LL_angularity(torch.tensor(xs), E0, R), color = \"black\", linestyle = \"--\", label = \"LL' Angularity\", alpha = 0.25)\n",
    "plt.plot(xs, LL_angularity(torch.tensor(xs), E0, R), color = \"black\", linestyle = \"--\", label = \"LL-exact Angularity\")\n",
    "\n",
    "\n",
    "# cutoff_norm = np.trapz(ps * cutoff, xs)\n",
    "# plt.plot(xs, cutoff * ps , color = \"grey\", label = rf\"Target (Cut at $c$ = {cs[0]})\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.ylim(1e-3, 1e3)\n",
    "\n",
    "c = torch.tensor((0.5,))\n",
    "samples = sigmoid(flow(c).sample((100000,)))\n",
    "x_samples = samples[:,0].detach().numpy()\n",
    "aux_samples = samples[:,1:].detach().numpy()\n",
    "\n",
    "\n",
    "# plot the auxiliary variables\n",
    "if auxiliary_params > 0:\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    for i in range(auxiliary_params):\n",
    "        plt.hist(aux_samples[:,i], bins=100, density=True, alpha = 0.5, label=f\"Aux {i}\")\n",
    "        prob = flow(c).log_prob(samples).exp()\n",
    "        # plt.scatter(aux_samples[:,i], prob.detach().numpy(), color = \"red\")\n",
    "    plt.legend()\n",
    "\n",
    "# correlation between auxiliary variable 1 and x\n",
    "fig, ax = plt.subplots(1,1)\n",
    "bar = ax.hist2d(x_samples, aux_samples[:,0], bins=100, density=True, norm=mpl.colors.LogNorm(), cmap=\"Reds\")\n",
    "plt.colorbar(bar[3])\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Aux 0\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21eda6c-1891-405e-a82b-458c862f6c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf881e7b-55c8-4060-a440-0aba45dc97fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da0584e-12a5-4c5e-8a5f-4a82937a2855",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
