{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from rikabplotlib.plot_utils import newplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Necessary for jax to work with 0^0\n",
    "def build_powers(base, length):\n",
    "   \n",
    "    def body_fun(i, arr):\n",
    "        # arr[i] = arr[i-1] * base\n",
    "        return arr.at[i].set(arr[i-1] * base)\n",
    "    \n",
    "    # Initialize an array of zeros, then set arr[0] = 1\n",
    "    arr = jnp.zeros((length,))\n",
    "    arr = arr.at[0].set(1.0)\n",
    "    \n",
    "    # fori_loop will fill in arr[1], arr[2], ... arr[length-1]\n",
    "    arr = jax.lax.fori_loop(1, length, body_fun, arr)\n",
    "    return arr\n",
    "\n",
    "@jax.jit\n",
    "def polynomial_f(t, alpha, params):\n",
    "\n",
    "    M, N = params.shape\n",
    "    \n",
    "    # Build powers of alpha: [1, alpha, alpha^2, ... alpha^(M-1)]\n",
    "    alpha_powers = build_powers(alpha, M)  # shape (M,)\n",
    "\n",
    "    # Build powers of t: [1, t, t^2, ... t^(N-1)]\n",
    "    t_powers = build_powers(t, N)         # shape (N,)\n",
    "\n",
    "    poly_val = alpha_powers @ params @ t_powers\n",
    "    return poly_val\n",
    "    \n",
    "\n",
    "\n",
    "def t_from_x(x):\n",
    "    return jnp.log(1/x)\n",
    "\n",
    "\n",
    "def construct_cdf(function):\n",
    "\n",
    "    def cdf(x, alpha, params):\n",
    "        t = t_from_x(x)\n",
    "        return jnp.nan_to_num(jnp.exp(-function(t, alpha, params)))\n",
    "    return cdf\n",
    "\n",
    "\n",
    "\n",
    "def construct_pdf(function):\n",
    "\n",
    "    cdf = construct_cdf(function)\n",
    "    derivative = jax.grad(cdf, argnums=0)\n",
    "\n",
    "    def pdf(x, alpha, params):\n",
    "        return jnp.nan_to_num(derivative(x, alpha, params) )\n",
    "\n",
    "    return pdf\n",
    "\n",
    "\n",
    "\n",
    "def taylor_expand_in_alpha(function, order):\n",
    "\n",
    "    ps = [function,]\n",
    "    if order > 0:\n",
    "        for i in range(order):\n",
    "            ps.append(jax.grad(ps[-1], argnums=1))\n",
    "\n",
    "    def taylor_expansion(x, alpha, params):\n",
    "        near_zero = 1e-16\n",
    "        terms = jnp.array([p(x, near_zero, params) for p in ps])\n",
    "        factorials = jax.scipy.special.gamma(jnp.arange(len(terms)) + 1)\n",
    "\n",
    "        return jnp.sum(terms / factorials * jnp.power(alpha, jnp.arange(len(terms))))\n",
    "    \n",
    "    return taylor_expansion\n",
    "\n",
    "\n",
    "\n",
    "def taylor_expand_in_t(function, order):\n",
    "\n",
    "    ps = [function,]\n",
    "    if order > 0:\n",
    "        for i in range(order):\n",
    "            ps.append(jax.grad(ps[-1], argnums=0))\n",
    "\n",
    "    def taylor_expansion(x, alpha, params):\n",
    "        near_zero = 1e-16\n",
    "        terms = jnp.array([p(near_zero, alpha, params) for p in ps])\n",
    "        factorials = jax.scipy.special.gamma(jnp.arange(len(terms)) + 1)\n",
    "\n",
    "        return jnp.sum(terms / factorials * jnp.power(x, jnp.arange(len(terms))))\n",
    "    \n",
    "    return taylor_expansion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(polynomial_f(5, 5, jnp.array([[1,1]])))\n",
    "\n",
    "derivative = jax.grad(polynomial_f, argnums=0)\n",
    "print(derivative(0.0, 0.0, jnp.array([[0,], [1,]])))\n",
    "\n",
    "derivative2 = jax.grad(derivative, argnums=0)\n",
    "print(derivative2(0.0, 0.0, jnp.array([[1,]])))\n",
    "\n",
    "derivative3 = jax.grad(derivative2, argnums=0)\n",
    "print(derivative3(0.0, 0.0, jnp.array([[1.0, 0.0] ])))\n",
    "\n",
    "# derivative4 = jax.grad(derivative3, argnums=0)\n",
    "# print(derivative4(0.0, 0.0, integral_coeffs[:2,:2]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40623/4110991313.py:61: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  c = jnp.zeros((M+1, N+1), dtype=jnp.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "[[0.  0.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.5 0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0.  0. ]]\n",
      "[[ 0.    -0.    -0.    -0.    -0.   ]\n",
      " [-0.    -0.     0.5   -0.    -0.   ]\n",
      " [-0.    -0.    -0.    -0.     0.125]\n",
      " [-0.    -0.    -0.    -0.    -0.   ]\n",
      " [-0.    -0.    -0.    -0.    -0.   ]]\n"
     ]
    }
   ],
   "source": [
    "def taylor_coefficients_2d(f, t0, alpha0, M, N, params=None):\n",
    "    \"\"\"\n",
    "    Compute the Taylor coefficients c[m, n] for the bivariate expansion of\n",
    "        f(t, alpha, params)\n",
    "    around (t0, alpha0).  We assume the expansion is about (0,0) if\n",
    "    t0=0, alpha0=0.\n",
    "\n",
    "    f_taylor(t, alpha) = sum_{m=0..M} sum_{n=0..N} c[m,n] * alpha^m * t^n\n",
    "    \n",
    "    where\n",
    "        c[m,n] = (1/(m! n!)) *\n",
    "                 (d^(m+n) f / d alpha^m d t^n)(alpha0, t0).\n",
    "    \n",
    "    This version builds a 2D table of partial-derivative functions so it \n",
    "    doesn't recompute derivatives from scratch for every (m, n). \n",
    "    (Much more efficient than the naive approach, for moderate M, N.)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f        : callable\n",
    "               A function f(t, alpha, params) -> scalar.\n",
    "    t0       : float\n",
    "               The point in t at which we expand.\n",
    "    alpha0   : float\n",
    "               The point in alpha at which we expand.\n",
    "    M        : int\n",
    "               Max power of alpha in the expansion.\n",
    "    N        : int\n",
    "               Max power of t in the expansion.\n",
    "    params   : any\n",
    "               Additional parameters that f may depend on.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    c : jnp.ndarray of shape (M+1, N+1)\n",
    "        The coefficients c[m, n].\n",
    "    \"\"\"\n",
    "    \n",
    "    # 0) Make a helper that has the right signature for JAX differentiation.\n",
    "    #    We assume f(t, alpha, params).  We’ll keep that interface for clarity.\n",
    "    def f_base(t, alpha):\n",
    "        return f(t, alpha, params)\n",
    "\n",
    "    # 1) Build partial derivatives wrt t in a list f_list[n] = (d^n/dt^n) f.\n",
    "    #    We'll do this by repeated application of jax.grad with argnums=0.\n",
    "    #\n",
    "    #    f_list[0](t, alpha) = f_base(t, alpha)\n",
    "    #    f_list[1](t, alpha) = d/dt of f_list[0], etc.\n",
    "    #\n",
    "    #    Each step is f_{(n+1)} = grad( f_{(n)}, argnums=0 ).\n",
    "    \n",
    "    f_list = [f_base]      # f_0\n",
    "    for n_ in range(N):\n",
    "        fn_plus_1 = jax.grad(f_list[-1], argnums=0)  # derivative wrt t\n",
    "        f_list.append(fn_plus_1)\n",
    "\n",
    "    # 2) For each f_list[n], we get all derivatives wrt alpha at alpha0,\n",
    "    #    up to order M.  We'll store them in c[m,n].\n",
    "    \n",
    "    # Initialize a JAX array for the coefficients.\n",
    "    c = jnp.zeros((M+1, N+1), dtype=jnp.float64)\n",
    "\n",
    "    def derivatives_wrt_alpha_up_to_order_M(fn_of_alpha, alpha0, M):\n",
    "        \"\"\"\n",
    "        Return [ fn_of_alpha^{(0)}(alpha0), \n",
    "                 fn_of_alpha^{(1)}(alpha0),\n",
    "                 ...\n",
    "                 fn_of_alpha^{(M)}(alpha0) ] \n",
    "        by repeated application of jax.grad wrt alpha (argnums=0).\n",
    "        \"\"\"\n",
    "        out = []\n",
    "        current_g = fn_of_alpha\n",
    "        for m_ in range(M+1):\n",
    "            if m_ == 0:\n",
    "                # 0th derivative => the function value\n",
    "                out.append(current_g(alpha0))\n",
    "            else:\n",
    "                # 1st..Mth => derivative wrt alpha\n",
    "                current_g = jax.grad(current_g, argnums=0)\n",
    "                out.append(current_g(alpha0))\n",
    "        return jnp.array(out)\n",
    "\n",
    "    # Loop over n=0..N\n",
    "    for n_ in range(N+1):\n",
    "        # The function f_n(t, alpha)\n",
    "        fn = f_list[n_]\n",
    "\n",
    "        # We'll evaluate partial derivatives wrt alpha of the function alpha -> fn(t0, alpha)\n",
    "        def fn_of_alpha(a):\n",
    "            return fn(t0, a)\n",
    "\n",
    "        # partials_alpha[m] = (d^m / dalpha^m) fn(t0, alpha) at alpha=alpha0\n",
    "        partials_alpha = derivatives_wrt_alpha_up_to_order_M(fn_of_alpha, alpha0, M)\n",
    "        # partials_alpha[m] = (∂^(m+n_)/∂t^n_ ∂alpha^m) f  at (t0, alpha0)\n",
    "\n",
    "        # 3) Fill c[m, n_] = partials_alpha[m] / (m! n_!)\n",
    "        for m_ in range(M+1):\n",
    "            val = partials_alpha[m_]\n",
    "            denom = math.factorial(m_) * math.factorial(n_)\n",
    "            c = c.at[m_, n_].set(val / denom)\n",
    "\n",
    "    return c\n",
    "\n",
    "\n",
    "def integrate_taylor_polynomial(c):\n",
    "  \n",
    "\n",
    "    M_plus_1, N_plus_1 = c.shape\n",
    "    M = M_plus_1 - 1\n",
    "    N = N_plus_1 - 1\n",
    "    \n",
    "    # New array will have shape (M+1, N+2)\n",
    "    d = np.zeros((M_plus_1, N_plus_1 + 1), dtype=c.dtype)\n",
    "    \n",
    "    # d[m,n] = c[m,n-1]/n, except d[m,0] = 0.\n",
    "    for m in range(M_plus_1):\n",
    "        for n in range(1, N+2):\n",
    "            d[m, n] = c[m, n-1] / n\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "def example_f(t, alpha, params):\n",
    "    return (alpha * t )   \n",
    "\n",
    "\n",
    "\n",
    "M = 5\n",
    "N = 5\n",
    "coeffs = taylor_coefficients_2d(example_f, 0.0, 0.0, M, N)\n",
    "integral_coeffs = integrate_taylor_polynomial(coeffs)\n",
    "\n",
    "def matching_coeffs(f, M, N):\n",
    "\n",
    "    coeffs = taylor_coefficients_2d(f, 0.0, 0.0, M, N)\n",
    "    integral_coeffs = integrate_taylor_polynomial(coeffs)\n",
    "\n",
    "    K = M + N\n",
    "\n",
    "    # @jax.jit\n",
    "    def temp(t, alpha, params):\n",
    "\n",
    "        x = polynomial_f(t, alpha, integral_coeffs)\n",
    "\n",
    "        # Compute -x - x2/2 - x3/3 - ... - xK/K = log(1-x)\n",
    "        return -1 * jnp.sum(jnp.array([x**k / k for k in range(1, K+1)]))\n",
    "\n",
    "    \n",
    "    return -1 * taylor_coefficients_2d(temp, 0.0, 0.0, M, N)\n",
    "\n",
    "# matched_coeffs = matching_coeffs(example_f, 3, 3)\n",
    "\n",
    "print(coeffs)\n",
    "print(integral_coeffs)\n",
    "print(matching_coeffs(example_f, 4, 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40623/4110991313.py:61: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  c = jnp.zeros((M+1, N+1), dtype=jnp.float64)\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.118\n",
    "\n",
    "x = jnp.linspace(0, 1, 10000)\n",
    "t = t_from_x(x)\n",
    "matched_coeffs = matching_coeffs(example_f, 4, 8)\n",
    "print(matched_coeffs)\n",
    "\n",
    "pdf = construct_pdf(polynomial_f)\n",
    "pdf = jax.vmap(pdf, in_axes=(0, None, None))\n",
    "\n",
    "fig, ax = newplot(\"full\")\n",
    "\n",
    "\n",
    "ax.plot(x, alpha * jnp.log(1/x) / x, color = 'black', ls = \"-\")\n",
    "# ax.plot(x, alpha * jnp.log(1/x) / x * jnp.exp( -alpha / 2 * jnp.log(1/x)**2), color = 'grey', ls = \"-\")\n",
    "\n",
    "\n",
    "\n",
    "# ax.plot(x, polynomial_f(t, alpha, coeffs) / x, label=\"Original\", color = \"black\")\n",
    "# ax.plot(x, pdf(x, alpha, matched_coeffs[:4 + 1]), label=r\"Matched $\\mathcal{O}(\\alpha_s^4)$\", color = \"red\")\n",
    "ax.plot(x, pdf(x, alpha, matched_coeffs[:3 + 4]), label=r\"Matched $\\mathcal{O}(\\alpha_s^3)$\", color = \"red\", alpha = 0.75)\n",
    "ax.plot(x, pdf(x, alpha, matched_coeffs[:2 + 1]), label=r\"Matched $\\mathcal{O}(\\alpha_s^2)$\", color = \"red\", alpha = 0.5)\n",
    "ax.plot(x, pdf(x, alpha, matched_coeffs[:1 + 1]), label=r\"Matched $\\mathcal{O}(\\alpha_s^1)$\", color = \"red\", alpha = 0.25)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(1e-3, 1e3)\n",
    "\n",
    "plt.legend(title=r\"$p(x|\\alpha) =\\alpha\\log(1/x)/x$\")\n",
    "\n",
    "\n",
    "# Plot the CDFs\n",
    "cdf = construct_cdf(polynomial_f)\n",
    "cdf = jax.vmap(cdf, in_axes=(0, None, None))\n",
    "\n",
    "fig, ax = newplot(\"full\")\n",
    "# ax.plot(x, polynomial_f(t, alpha, integral_coeffs), label=\"Original\", color = \"black\")\n",
    "ax.plot(x, cdf(x, alpha, matched_coeffs), label=r\"Matched $\\mathcal{O}(\\alpha_s^3)$\", color = \"red\")\n",
    "ax.plot(x, cdf(x, alpha, matched_coeffs[:-1]), label=r\"Matched $\\mathcal{O}(\\alpha_s^2)$\", color = \"red\", alpha = 0.5)\n",
    "ax.plot(x, cdf(x, alpha, matched_coeffs[:-2]), label=r\"Matched $\\mathcal{O}(\\alpha_s^1)$\", color = \"red\", alpha = 0.25)\n",
    "\n",
    "# ax.plot(x, 1- alpha * jnp.log(1/x) ** 2, color = 'black', ls = \"-\")\n",
    "ax.plot(x, jnp.exp( -alpha / 2 * jnp.log(1/x)**2), color = 'pink', ls = \"--\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SHAPER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
